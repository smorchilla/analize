{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###vse\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Zero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sklearn\n",
    "import codecs\n",
    "import pymorphy2\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('russian'))\n",
    "\n",
    "def csv_to_list(arr):\n",
    "    arr_list = []\n",
    "    for row in arr:\n",
    "        arr_list.append(list_to_str(row))\n",
    "    return arr_list\n",
    "\n",
    "def list_to_str(arr):\n",
    "    str_ = ''\n",
    "    for rec in arr:\n",
    "        str_+=rec\n",
    "    return str_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_recalls = csv_to_list(csv.reader(codecs.open(r'C:\\Users\\Zero\\Downloads\\negative.csv', 'rU', 'utf-8', errors='ignore')))\n",
    "negative_recalls = csv_to_list(csv.reader(codecs.open(r'C:\\Users\\Zero\\Downloads\\positive.csv', 'rU', 'utf-8', errors='ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>408907E+17;1386325944;dugarchikbellko;на работ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>408907E+17;1386325957;nugemycejela;Коллеги сид...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>408907E+17;1386325966;4post21;@elina_4post как...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>408907E+17;1386325980;Poliwake;Желаю хорошего ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>408907E+17;1386325980;capyvixowe;Обновил за ка...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              recall\n",
       "0  408907E+17;1386325944;dugarchikbellko;на работ...\n",
       "1  408907E+17;1386325957;nugemycejela;Коллеги сид...\n",
       "2  408907E+17;1386325966;4post21;@elina_4post как...\n",
       "3  408907E+17;1386325980;Poliwake;Желаю хорошего ...\n",
       "4  408907E+17;1386325980;capyvixowe;Обновил за ка..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_positive_recalls = pd.DataFrame(positive_recalls+negative_recalls, columns=['recall'])\n",
    "\n",
    "df_positive_recalls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tokenize everything: 0.03 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>408907E+17;1386325944;dugarchikbellko;на работ...</td>\n",
       "      <td>408907E+17;1386325944;dugarchikbellko;на работ...</td>\n",
       "      <td>[408907E, 17, 1386325944, dugarchikbellko, на,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>408907E+17;1386325957;nugemycejela;Коллеги сид...</td>\n",
       "      <td>408907E+17;1386325957;nugemycejela;Коллеги сид...</td>\n",
       "      <td>[408907E, 17, 1386325957, nugemycejela, Коллег...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>408907E+17;1386325966;4post21;@elina_4post как...</td>\n",
       "      <td>408907E+17;1386325966;4post21;@elina_4post как...</td>\n",
       "      <td>[408907E, 17, 1386325966, 4post21, elina_4post...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>408907E+17;1386325980;Poliwake;Желаю хорошего ...</td>\n",
       "      <td>408907E+17;1386325980;Poliwake;Желаю хорошего ...</td>\n",
       "      <td>[408907E, 17, 1386325980, Poliwake, Желаю, хор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>408907E+17;1386325980;capyvixowe;Обновил за ка...</td>\n",
       "      <td>408907E+17;1386325980;capyvixowe;Обновил за ка...</td>\n",
       "      <td>[408907E, 17, 1386325980, capyvixowe, Обновил,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              recall  \\\n",
       "0  408907E+17;1386325944;dugarchikbellko;на работ...   \n",
       "1  408907E+17;1386325957;nugemycejela;Коллеги сид...   \n",
       "2  408907E+17;1386325966;4post21;@elina_4post как...   \n",
       "3  408907E+17;1386325980;Poliwake;Желаю хорошего ...   \n",
       "4  408907E+17;1386325980;capyvixowe;Обновил за ка...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  408907E+17;1386325944;dugarchikbellko;на работ...   \n",
       "1  408907E+17;1386325957;nugemycejela;Коллеги сид...   \n",
       "2  408907E+17;1386325966;4post21;@elina_4post как...   \n",
       "3  408907E+17;1386325980;Poliwake;Желаю хорошего ...   \n",
       "4  408907E+17;1386325980;capyvixowe;Обновил за ка...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [408907E, 17, 1386325944, dugarchikbellko, на,...  \n",
       "1  [408907E, 17, 1386325957, nugemycejela, Коллег...  \n",
       "2  [408907E, 17, 1386325966, 4post21, elina_4post...  \n",
       "3  [408907E, 17, 1386325980, Poliwake, Желаю, хор...  \n",
       "4  [408907E, 17, 1386325980, capyvixowe, Обновил,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_positive_recalls['clean'] = df_positive_recalls['recall'].astype('str') \n",
    "df_positive_recalls.dtypes\n",
    "\n",
    "df_positive_recalls[\"tokens\"] = df_positive_recalls['recall'].apply(tokenizer.tokenize)\n",
    "# delete Stop Words\n",
    "\n",
    "print('Time to tokenize everything: {} mins'.format(round((time.time() - t) / 60, 2)))\n",
    "df_positive_recalls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Zero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('russian')\n",
    "stopwords\n",
    "\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "df_clean = df_positive_recalls['clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'а',\n",
       " 'без',\n",
       " 'более',\n",
       " 'больше',\n",
       " 'будет',\n",
       " 'будто',\n",
       " 'бы',\n",
       " 'был',\n",
       " 'была',\n",
       " 'были',\n",
       " 'было',\n",
       " 'быть',\n",
       " 'в',\n",
       " 'вам',\n",
       " 'вас',\n",
       " 'вдруг',\n",
       " 'ведь',\n",
       " 'во',\n",
       " 'вот',\n",
       " 'впрочем',\n",
       " 'все',\n",
       " 'всегда',\n",
       " 'всего',\n",
       " 'всех',\n",
       " 'всю',\n",
       " 'вы',\n",
       " 'где',\n",
       " 'да',\n",
       " 'даже',\n",
       " 'два',\n",
       " 'для',\n",
       " 'до',\n",
       " 'другой',\n",
       " 'его',\n",
       " 'ее',\n",
       " 'ей',\n",
       " 'ему',\n",
       " 'если',\n",
       " 'есть',\n",
       " 'еще',\n",
       " 'ж',\n",
       " 'же',\n",
       " 'за',\n",
       " 'зачем',\n",
       " 'здесь',\n",
       " 'и',\n",
       " 'из',\n",
       " 'или',\n",
       " 'им',\n",
       " 'иногда',\n",
       " 'их',\n",
       " 'к',\n",
       " 'как',\n",
       " 'какая',\n",
       " 'какой',\n",
       " 'когда',\n",
       " 'конечно',\n",
       " 'кто',\n",
       " 'куда',\n",
       " 'ли',\n",
       " 'лучше',\n",
       " 'между',\n",
       " 'меня',\n",
       " 'мне',\n",
       " 'много',\n",
       " 'может',\n",
       " 'можно',\n",
       " 'мой',\n",
       " 'моя',\n",
       " 'мы',\n",
       " 'на',\n",
       " 'над',\n",
       " 'надо',\n",
       " 'наконец',\n",
       " 'нас',\n",
       " 'не',\n",
       " 'него',\n",
       " 'нее',\n",
       " 'ней',\n",
       " 'нельзя',\n",
       " 'нет',\n",
       " 'ни',\n",
       " 'нибудь',\n",
       " 'никогда',\n",
       " 'ним',\n",
       " 'них',\n",
       " 'ничего',\n",
       " 'но',\n",
       " 'ну',\n",
       " 'о',\n",
       " 'об',\n",
       " 'один',\n",
       " 'он',\n",
       " 'она',\n",
       " 'они',\n",
       " 'опять',\n",
       " 'от',\n",
       " 'перед',\n",
       " 'по',\n",
       " 'под',\n",
       " 'после',\n",
       " 'потом',\n",
       " 'потому',\n",
       " 'почти',\n",
       " 'при',\n",
       " 'про',\n",
       " 'раз',\n",
       " 'разве',\n",
       " 'с',\n",
       " 'сам',\n",
       " 'свою',\n",
       " 'себе',\n",
       " 'себя',\n",
       " 'сейчас',\n",
       " 'со',\n",
       " 'совсем',\n",
       " 'так',\n",
       " 'такой',\n",
       " 'там',\n",
       " 'тебя',\n",
       " 'тем',\n",
       " 'теперь',\n",
       " 'то',\n",
       " 'тогда',\n",
       " 'того',\n",
       " 'тоже',\n",
       " 'только',\n",
       " 'том',\n",
       " 'тот',\n",
       " 'три',\n",
       " 'тут',\n",
       " 'ты',\n",
       " 'у',\n",
       " 'уж',\n",
       " 'уже',\n",
       " 'хорошо',\n",
       " 'хоть',\n",
       " 'чего',\n",
       " 'чем',\n",
       " 'через',\n",
       " 'что',\n",
       " 'чтоб',\n",
       " 'чтобы',\n",
       " 'чуть',\n",
       " 'эти',\n",
       " 'этого',\n",
       " 'этой',\n",
       " 'этом',\n",
       " 'этот',\n",
       " 'эту',\n",
       " 'я'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         408907E+17;1386325944;dugarchikbellko;на работ...\n",
       "1         408907E+17;1386325957;nugemycejela;Коллеги сид...\n",
       "2         408907E+17;1386325966;4post21;@elina_4post гов...\n",
       "3         408907E+17;1386325980;Poliwake;Желаю хорошего ...\n",
       "4         408907E+17;1386325980;capyvixowe;Обновил каким...\n",
       "                                ...                        \n",
       "275813    411369E+17;1386912922;diminlisenok;Спала родит...\n",
       "275814    411369E+17;1386912922;qilepocagotu;RT @jebesil...\n",
       "275815    411369E+17;1386912938;DennyChooo;Что происходи...\n",
       "275816    411369E+17;1386912938;bedowabymir;\"\"\"Любимаяя ...\n",
       "275817    411369E+17;1386912953;Prituljak_Sibir;@Ma_che_...\n",
       "Name: clean, Length: 275818, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hero=df_clean.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "patterns = \"[0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "\n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, '', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        new_token=''\n",
    "        for char in token:\n",
    "            if (char not in 'abcdefghijklmnopqrstuvwxyz') and (char not in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'):\n",
    "                new_token = str(new_token) + str(char)\n",
    "        token=new_token\n",
    "        if token:\n",
    "            token = token.strip()\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=hero.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [на, работе, полный, пиддес, каждое, закрытие,...\n",
       "1         [коллеги, сидят, рубятся, изза, долбанной, вин...\n",
       "2                          [говорят, обещаного, года, ждут]\n",
       "3         [желаю, хорошего, полёта, удачной, посадкия, б...\n",
       "4          [обновил, какимто, лешим, работает, простоплеер]\n",
       "                                ...                        \n",
       "275813    [спала, родительском, доме, своей, кровати, пр...\n",
       "275814    [эх, мы, немного, решили, сократить, путь, леж...\n",
       "275815    [что, происходит, мной, эфире, звучит, любимая...\n",
       "275816    [любимаяя, подарю, тебе, звезду, имя, звезды, ...\n",
       "275817    [посмотри, непытайтесьпокинутьомск, сегодня, в...\n",
       "Name: clean, Length: 275818, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.07 mins\n",
      "Time to train the model: 0.44 mins\n"
     ]
    }
   ],
   "source": [
    "    import multiprocessing\n",
    "import time\n",
    "#WORD2VEC()\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=10,\n",
    "    window=4,\n",
    "    size=300,\n",
    "    negative=10,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    sample=6e-5,\n",
    "    sg=1,workers=4)\n",
    "\n",
    "#BUILD_VOCAB()\n",
    "t = time.time()\n",
    "w2v_model.build_vocab(data, progress_per=1000)\n",
    "print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))\n",
    "\n",
    "#TRAIN()\n",
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=10, report_delay=1)\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "import  gensim as pd\n",
    "print (pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.wv.save_word2vec_format('exp_model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne_scatterplot(model, word, list_names):\n",
    "    \"\"\"Plot in seaborn the results from the t-SNE dimensionality reduction \n",
    "    algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\"\"\"\n",
    "    vectors_words = [model.wv.word_vec(word)]\n",
    "    word_labels = [word]\n",
    "    color_list = ['red']\n",
    "    close_words = model.wv.most_similar(word)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.word_vec(wrd_score[0])\n",
    "        vectors_words.append(wrd_vector)\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.word_vec(wrd)\n",
    "        vectors_words.append(wrd_vector)\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "    # t-SNE reduction\n",
    "    Y = (TSNE(n_components=2, random_state=0, perplexity=15, init=\"pca\")\n",
    "        .fit_transform(vectors_words))\n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({\"x\": [x for x in Y[:, 0]],\n",
    "                    \"y\": [y for y in Y[:, 1]],\n",
    "                    \"words\": word_labels,\n",
    "                    \"color\": color_list})\n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                    x=\"x\",\n",
    "                    y=\"y\",\n",
    "                    fit_reg=False,\n",
    "                    marker=\"o\",\n",
    "                    scatter_kws={\"s\": 40,\n",
    "                                \"facecolors\": df[\"color\"]}\n",
    "    )\n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "        p1.text(df[\"x\"][line],\n",
    "                df[\"y\"][line],\n",
    "                \" \" + df[\"words\"][line].title(),\n",
    "                horizontalalignment=\"left\",\n",
    "                verticalalignment=\"bottom\", size=\"medium\",\n",
    "                color=df[\"color\"][line],\n",
    "                weight=\"normal\"\n",
    "        ).set_size(15)\n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gensim' has no attribute 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b3ad81911b64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtsne_scatterplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"образование\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"наука\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"россия\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"информация\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-05408b0509a0>\u001b[0m in \u001b[0;36mtsne_scatterplot\u001b[1;34m(model, word, list_names)\u001b[0m\n\u001b[0;32m     26\u001b[0m         .fit_transform(vectors_words))\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Sets everything up to plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     df = pd.DataFrame({\"x\": [x for x in Y[:, 0]],\n\u001b[0m\u001b[0;32m     29\u001b[0m                     \u001b[1;34m\"y\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                     \u001b[1;34m\"words\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mword_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gensim' has no attribute 'DataFrame'"
     ]
    }
   ],
   "source": [
    "tsne_scatterplot(w2v_model, \"образование\", [\"наука\", \"россия\", \"информация\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-c8ac0a104979>:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model1.syn0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.11728935,  0.01785734, -0.01485978, ...,  0.03671508,\n",
       "         0.04328277,  0.05702804],\n",
       "       [ 0.02634426, -0.06127802, -0.06673258, ...,  0.0326159 ,\n",
       "        -0.04336162,  0.02965805],\n",
       "       [-0.00874856, -0.03064145, -0.03029749, ...,  0.05440014,\n",
       "        -0.02184396,  0.03046235],\n",
       "       ...,\n",
       "       [-0.05809074, -0.08248737,  0.00392503, ...,  0.10255912,\n",
       "         0.01462533,  0.13502745],\n",
       "       [-0.03572841, -0.06809242,  0.04669642, ...,  0.10245778,\n",
       "        -0.01824561,  0.11503725],\n",
       "       [-0.05724758, -0.05399029, -0.03043254, ...,  0.15535876,\n",
       "         0.01410423,  0.11516733]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = w2v_model.wv.load_word2vec_format(r'C:\\Users\\Zero\\exp_model.bin', binary=True)\n",
    "model1.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-86cc11a75b55>:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  word_vectors = model1.syn0# here you load vectors for each word in your model\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model1.syn0# here you load vectors for each word in your model\n",
    "n_words = word_vectors.shape[0]\n",
    "vec_size = word_vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#words = 16794, vector size = 300\n"
     ]
    }
   ],
   "source": [
    "print(\"#words = {0}, vector size = {1}\".format(n_words, vec_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute clustering ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zero\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:938: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished in 4.02 sec.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Compute clustering ... \", end=\"\", flush=True)\n",
    "kmeans = KMeans(n_clusters=10, n_jobs=-1, random_state=0)\n",
    "idx = kmeans.fit_predict(word_vectors)\n",
    "print(\"finished in {:.2f} sec.\".format(time.time() - start), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Принадлежность к кластерам:\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(\"Принадлежность к кластерам:\\n{}\".format(kmeans.labels_[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-3a66b657906f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "w2v_model.wv.word_vec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.most_similar(positive=[\"день\", \"завтра\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-37b38b84b612>:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  index2word_set = set(model1.wv.index2word)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "index2word_set = set(model1.wv.index2word)\n",
    "\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8628910779953003\n"
     ]
    }
   ],
   "source": [
    "s1_afv = avg_feature_vector('Конкурс открывает талантливым управленцам путь для продвижения на высокие позиции»', model=model1, num_features=300, index2word_set=index2word_set)\n",
    "s2_afv = avg_feature_vector('В Мастерской управления «Сенеж» завершил работу V Всероссийский форум волонтеров-медиков', model=model1, num_features=300, index2word_set=index2word_set)\n",
    "sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'33 человека стали победителями «Конкурса политологов» / история': 0.3251340091228485, '33 человека стали победителями «Конкурса политологов» / политика': 0.4133995473384857, '33 человека стали победителями «Конкурса политологов» / наука': 0.629466712474823, '33 человека стали победителями «Конкурса политологов» / образование': 0.7052112221717834}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "listok=['наука','образование','история','политика']\n",
    "deist='33 человека стали победителями «Конкурса политологов»'\n",
    "score=[]\n",
    "slovar={}\n",
    "for i in range(len(listok)):\n",
    "    s1_afv = avg_feature_vector('33 человека стали победителями «Конкурса политологов»', model=model1, num_features=300, index2word_set=index2word_set)\n",
    "    s2_afv = avg_feature_vector(listok[i], model=model1, num_features=300, index2word_set=index2word_set)\n",
    "    sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "    score.append(sim)\n",
    "    slovar.update({deist+' / '+listok[i]:sim})\n",
    "sorted_tuples = sorted(slovar.items(), key=lambda item: item[1])\n",
    "\n",
    "sorted_dict = {k: v for k, v in sorted_tuples}\n",
    "\n",
    "print(sorted_dict)  # {1: 1, 3: 4, 2: 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
