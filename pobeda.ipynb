{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhiti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from scipy import spatial\n",
    "import time\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel(r\"datasets/extremism.xlsx\")\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['id'] = df1['id']\n",
    "df['text'] = df1['text']\n",
    "df['class'] = df1['class']\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Таким образом никто не внес большего вклад...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Можно с уверенностью сказать, что все полезное...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Славяне должны на нас работать. Если они нам б...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Русское государство росло, развивалось из свои...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Поставим власть под контроль народа</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>305</td>\n",
       "      <td>Социальное. Соответствующий подход предполагае...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>306</td>\n",
       "      <td>Рост продуктивности человеческой деятельности ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>307</td>\n",
       "      <td>Социальная дифференциация, в том числе по этни...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>308</td>\n",
       "      <td>Демографический рост и расширение экономическо...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>309</td>\n",
       "      <td>Соответственно, политика возникает в связи с у...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  class\n",
       "0      1      Таким образом никто не внес большего вклад...    3.0\n",
       "1      2  Можно с уверенностью сказать, что все полезное...    3.0\n",
       "2      3  Славяне должны на нас работать. Если они нам б...    3.0\n",
       "3      4  Русское государство росло, развивалось из свои...    3.0\n",
       "4      5                Поставим власть под контроль народа    3.0\n",
       "..   ...                                                ...    ...\n",
       "304  305  Социальное. Соответствующий подход предполагае...    0.0\n",
       "305  306  Рост продуктивности человеческой деятельности ...    0.0\n",
       "306  307  Социальная дифференциация, в том числе по этни...    0.0\n",
       "307  308  Демографический рост и расширение экономическо...    0.0\n",
       "308  309  Соответственно, политика возникает в связи с у...    0.0\n",
       "\n",
       "[309 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    min_count=10,\n",
    "    window=10,\n",
    "    size=300,\n",
    "    negative=10,\n",
    "    alpha=0.5,\n",
    "    min_alpha=0.0007,\n",
    "    sample=6e-5,\n",
    "    sg=1,workers=4)\n",
    "\n",
    "exp_model = w2v_model.wv.load_word2vec_format(r'models/exp_model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('соседка', 0.8491843938827515),\n",
       " ('собачка', 0.8448320031166077),\n",
       " ('леся', 0.841035008430481),\n",
       " ('насквозь', 0.817664623260498),\n",
       " ('болеет', 0.8155531883239746),\n",
       " ('пьяная', 0.8155224323272705),\n",
       " ('бедная', 0.8139984011650085),\n",
       " ('тётя', 0.8106111288070679),\n",
       " ('мелкая', 0.8079123497009277),\n",
       " ('достала', 0.8047630786895752)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_model.most_similar('кошка')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = \"[0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "\n",
    "def comma2space(word):\n",
    "    new_word = ''\n",
    "    try:\n",
    "        for char in word:\n",
    "            if char in patterns:\n",
    "                char = ' '\n",
    "            new_word += char\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return new_word\n",
    "    \n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, '', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token:\n",
    "            \n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def dropstopwords(docs):\n",
    "    stopwords = nltk.corpus.stopwords.words('russian')\n",
    "    sen =  []\n",
    "    for doc in docs:\n",
    "        if (doc not in stopwords):\n",
    "            sen.append(doc)\n",
    "    return sen\n",
    "\n",
    "def dropcomma(doc):\n",
    "    new_sen = ''\n",
    "    for char in doc:\n",
    "        if (char not in patterns):\n",
    "            new_sen += char\n",
    "    return new_sen\n",
    "\n",
    "def tokenize(docs):\n",
    "    docs = comma2space(docs)\n",
    "    docs = lemmatize(docs)\n",
    "    docs = dropstopwords(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium word-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediumVector(sen):\n",
    "    medVec = exp_model[\"хачи\"]\n",
    "    unexceptedVectors = 0\n",
    "    for word in sen:\n",
    "        vec = False\n",
    "        try:\n",
    "            vec = exp_model.word_vec(word)\n",
    "            medVec = medVec + vec\n",
    "        except:\n",
    "            unexceptedVectors += 1\n",
    "            \n",
    "    if unexceptedVectors != 0 :\n",
    "        print('Lost - ' + str(unexceptedVectors))\n",
    "    elif unexceptedVectors == len(sen)-1:\n",
    "        print('Sentence lost')\n",
    "        \n",
    "    medVec = medVec - exp_model[\"хачи\"]\n",
    "    medVec = medVec / (len(sen) - unexceptedVectors)\n",
    "    return medVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [таким, образом, никто, внес, большего, вклада...\n",
       "1      [уверенностью, сказать, полезное, хорошее, сов...\n",
       "2      [славяне, должны, работать, нам, нужны, могут,...\n",
       "3      [русское, государство, росло, развивалось, сво...\n",
       "4                   [поставим, власть, контроль, народа]\n",
       "                             ...                        \n",
       "304    [социальное, соответствующий, подход, предпола...\n",
       "305    [рост, продуктивности, человеческой, деятельно...\n",
       "306    [социальная, дифференциация, числе, этническом...\n",
       "307    [демографический, рост, расширение, экономичес...\n",
       "308    [соответственно, политика, возникает, связи, у...\n",
       "Name: text, Length: 309, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df['text'].apply(tokenize)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost - 6\n",
      "Lost - 4\n",
      "Lost - 11\n",
      "Lost - 10\n",
      "Lost - 1\n",
      "Lost - 1\n",
      "Lost - 1\n",
      "Lost - 2\n",
      "Lost - 42\n",
      "Lost - 6\n",
      "Lost - 11\n",
      "Lost - 6\n",
      "Lost - 2\n",
      "Lost - 7\n",
      "Lost - 6\n",
      "Lost - 5\n",
      "Lost - 7\n",
      "Lost - 118\n",
      "Lost - 5\n",
      "Lost - 5\n",
      "Lost - 7\n",
      "Lost - 2\n",
      "Lost - 4\n",
      "Lost - 14\n",
      "Lost - 5\n",
      "Lost - 10\n",
      "Lost - 6\n",
      "Lost - 2\n",
      "Lost - 6\n",
      "Lost - 5\n",
      "Lost - 6\n",
      "Lost - 4\n",
      "Lost - 1\n",
      "Lost - 1\n",
      "Lost - 3\n",
      "Lost - 7\n",
      "Lost - 16\n",
      "Lost - 6\n",
      "Lost - 17\n",
      "Lost - 3\n",
      "Lost - 1\n",
      "Lost - 6\n",
      "Lost - 3\n",
      "Lost - 4\n",
      "Lost - 1\n",
      "Lost - 9\n",
      "Lost - 7\n",
      "Lost - 4\n",
      "Lost - 4\n",
      "Lost - 9\n",
      "Lost - 6\n",
      "Lost - 4\n",
      "Lost - 3\n",
      "Lost - 5\n",
      "Lost - 3\n",
      "Lost - 8\n",
      "Lost - 4\n",
      "Lost - 3\n",
      "Lost - 1\n",
      "Lost - 6\n",
      "Lost - 1\n",
      "Lost - 1\n",
      "Lost - 1\n",
      "Lost - 2\n",
      "Lost - 2\n",
      "Lost - 3\n",
      "Lost - 12\n",
      "Lost - 4\n",
      "Lost - 2\n",
      "Lost - 6\n",
      "Lost - 4\n",
      "Lost - 3\n",
      "Lost - 3\n",
      "Lost - 8\n",
      "Lost - 3\n",
      "Lost - 2\n",
      "Lost - 5\n",
      "Lost - 2\n",
      "Lost - 1\n",
      "Lost - 4\n",
      "Lost - 2\n",
      "Lost - 1\n",
      "Lost - 2\n",
      "Lost - 3\n",
      "Lost - 5\n",
      "Lost - 1\n",
      "Lost - 1\n",
      "Lost - 3\n",
      "Lost - 7\n",
      "Lost - 9\n",
      "Lost - 14\n",
      "Lost - 5\n",
      "Lost - 4\n",
      "Lost - 6\n",
      "Lost - 5\n",
      "Lost - 7\n",
      "Lost - 5\n",
      "Lost - 4\n",
      "Lost - 10\n",
      "Lost - 5\n",
      "Lost - 1\n",
      "Lost - 1\n",
      "Lost - 50\n",
      "Lost - 6\n",
      "Lost - 11\n",
      "Lost - 11\n",
      "Lost - 7\n",
      "Lost - 1\n",
      "Lost - 5\n",
      "Lost - 7\n",
      "Lost - 4\n",
      "Lost - 1\n",
      "Lost - 9\n",
      "Lost - 5\n",
      "Lost - 9\n",
      "Lost - 10\n",
      "Lost - 8\n",
      "Lost - 2\n",
      "Lost - 2\n",
      "Lost - 2\n",
      "Lost - 1\n",
      "Lost - 4\n",
      "Lost - 6\n",
      "Lost - 3\n",
      "Lost - 14\n",
      "Lost - 7\n",
      "Lost - 6\n",
      "Lost - 9\n",
      "Lost - 1\n",
      "Lost - 2\n",
      "Lost - 10\n",
      "Lost - 28\n",
      "Lost - 23\n",
      "Lost - 3\n",
      "Lost - 4\n",
      "Lost - 12\n",
      "Lost - 12\n",
      "Lost - 9\n",
      "Lost - 10\n",
      "Lost - 13\n",
      "Lost - 2\n",
      "Lost - 1\n",
      "Lost - 4\n",
      "Lost - 4\n",
      "Lost - 1\n",
      "Lost - 11\n",
      "Lost - 18\n",
      "Lost - 18\n",
      "Lost - 6\n",
      "Lost - 3\n",
      "Lost - 6\n",
      "Lost - 8\n",
      "Lost - 5\n",
      "Lost - 3\n",
      "Lost - 5\n",
      "Lost - 4\n",
      "Lost - 8\n",
      "Lost - 7\n",
      "Lost - 2\n",
      "Lost - 6\n",
      "Lost - 4\n",
      "Lost - 1\n",
      "Lost - 2\n",
      "Lost - 3\n",
      "Lost - 3\n",
      "Lost - 2\n",
      "Lost - 2\n",
      "Lost - 3\n",
      "Lost - 3\n",
      "Lost - 5\n",
      "Lost - 2\n",
      "Lost - 8\n",
      "Lost - 3\n",
      "Lost - 1\n",
      "Lost - 1\n",
      "Lost - 5\n",
      "Lost - 6\n",
      "Lost - 14\n",
      "Lost - 3\n",
      "Lost - 23\n",
      "Lost - 3\n",
      "Lost - 4\n",
      "Lost - 2\n",
      "Lost - 8\n",
      "Lost - 4\n",
      "Lost - 8\n",
      "Lost - 1\n",
      "Lost - 5\n",
      "Lost - 1\n",
      "Lost - 3\n",
      "Lost - 15\n",
      "Lost - 4\n",
      "Lost - 2\n",
      "Lost - 7\n",
      "Lost - 10\n",
      "Lost - 11\n",
      "Lost - 1\n",
      "Lost - 61\n",
      "Lost - 50\n",
      "Lost - 37\n",
      "Lost - 31\n",
      "Lost - 57\n",
      "Lost - 45\n",
      "Lost - 60\n",
      "Lost - 69\n",
      "Lost - 38\n",
      "Lost - 33\n",
      "Lost - 34\n",
      "Lost - 26\n",
      "Lost - 21\n",
      "Lost - 30\n",
      "Lost - 37\n",
      "Lost - 32\n",
      "Lost - 14\n",
      "Lost - 104\n",
      "Lost - 23\n",
      "Lost - 33\n",
      "Lost - 4\n",
      "Lost - 35\n",
      "Lost - 33\n",
      "Lost - 16\n",
      "Lost - 16\n",
      "Lost - 37\n",
      "Lost - 31\n",
      "Lost - 38\n",
      "Lost - 34\n",
      "Lost - 32\n",
      "Lost - 25\n",
      "Lost - 48\n",
      "Lost - 30\n",
      "Lost - 40\n",
      "Lost - 21\n",
      "Lost - 22\n",
      "Lost - 16\n",
      "Lost - 35\n",
      "Lost - 46\n",
      "Lost - 27\n",
      "Lost - 10\n",
      "Lost - 23\n",
      "Lost - 27\n",
      "Lost - 55\n",
      "Lost - 25\n",
      "Lost - 26\n",
      "Lost - 37\n",
      "Lost - 26\n",
      "Lost - 11\n",
      "Lost - 17\n",
      "Lost - 31\n",
      "Lost - 14\n",
      "Lost - 53\n",
      "Lost - 25\n",
      "Lost - 35\n",
      "Lost - 21\n",
      "Lost - 15\n",
      "Lost - 29\n",
      "Lost - 25\n",
      "Lost - 23\n",
      "Lost - 13\n",
      "Lost - 9\n",
      "Lost - 17\n",
      "Lost - 22\n",
      "Lost - 18\n",
      "Lost - 35\n",
      "Lost - 28\n",
      "Lost - 8\n",
      "Lost - 36\n",
      "Lost - 10\n",
      "Lost - 3\n",
      "Lost - 34\n",
      "Lost - 36\n",
      "Lost - 4\n",
      "Lost - 21\n",
      "Lost - 4\n",
      "Lost - 15\n",
      "Lost - 22\n",
      "Lost - 7\n",
      "Lost - 17\n",
      "Lost - 10\n",
      "Lost - 8\n",
      "Lost - 19\n",
      "Lost - 15\n",
      "Lost - 12\n",
      "Lost - 23\n",
      "Lost - 19\n",
      "Lost - 23\n",
      "Lost - 17\n",
      "Lost - 22\n",
      "Lost - 22\n",
      "Lost - 26\n",
      "Lost - 15\n",
      "Lost - 20\n",
      "Lost - 6\n",
      "Lost - 4\n",
      "Lost - 14\n",
      "Lost - 41\n",
      "Lost - 17\n",
      "Lost - 6\n",
      "Lost - 14\n",
      "Lost - 17\n",
      "Lost - 16\n",
      "Lost - 15\n",
      "Lost - 25\n",
      "Lost - 5\n",
      "Lost - 14\n",
      "Lost - 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>medVec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Таким образом никто не внес большего вклад...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[таким, образом, никто, внес, большего, вклада...</td>\n",
       "      <td>[-0.040627204, -0.015365396, -0.047234748, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Можно с уверенностью сказать, что все полезное...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[уверенностью, сказать, полезное, хорошее, сов...</td>\n",
       "      <td>[-0.039338265, -0.028398223, -0.0673505, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Славяне должны на нас работать. Если они нам б...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[славяне, должны, работать, нам, нужны, могут,...</td>\n",
       "      <td>[-0.036592077, -0.06395722, -0.045187872, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Русское государство росло, развивалось из свои...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[русское, государство, росло, развивалось, сво...</td>\n",
       "      <td>[-0.01085669, -0.022112658, -0.07141073, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Поставим власть под контроль народа</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[поставим, власть, контроль, народа]</td>\n",
       "      <td>[-0.052434314, -0.07669518, -0.019612849, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>305</td>\n",
       "      <td>Социальное. Соответствующий подход предполагае...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[социальное, соответствующий, подход, предпола...</td>\n",
       "      <td>[-0.03102859, -0.050743006, -0.037808374, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>306</td>\n",
       "      <td>Рост продуктивности человеческой деятельности ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[рост, продуктивности, человеческой, деятельно...</td>\n",
       "      <td>[-0.022473654, -0.052780617, -0.06827721, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>307</td>\n",
       "      <td>Социальная дифференциация, в том числе по этни...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[социальная, дифференциация, числе, этническом...</td>\n",
       "      <td>[-0.017352613, -0.019620007, -0.04677039, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>308</td>\n",
       "      <td>Демографический рост и расширение экономическо...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[демографический, рост, расширение, экономичес...</td>\n",
       "      <td>[-0.03736097, -0.06539034, -0.062378306, -0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>309</td>\n",
       "      <td>Соответственно, политика возникает в связи с у...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[соответственно, политика, возникает, связи, у...</td>\n",
       "      <td>[-0.048907045, -0.052896462, -0.05555214, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  class  \\\n",
       "0      1      Таким образом никто не внес большего вклад...    3.0   \n",
       "1      2  Можно с уверенностью сказать, что все полезное...    3.0   \n",
       "2      3  Славяне должны на нас работать. Если они нам б...    3.0   \n",
       "3      4  Русское государство росло, развивалось из свои...    3.0   \n",
       "4      5                Поставим власть под контроль народа    3.0   \n",
       "..   ...                                                ...    ...   \n",
       "304  305  Социальное. Соответствующий подход предполагае...    0.0   \n",
       "305  306  Рост продуктивности человеческой деятельности ...    0.0   \n",
       "306  307  Социальная дифференциация, в том числе по этни...    0.0   \n",
       "307  308  Демографический рост и расширение экономическо...    0.0   \n",
       "308  309  Соответственно, политика возникает в связи с у...    0.0   \n",
       "\n",
       "                                             tokenized  \\\n",
       "0    [таким, образом, никто, внес, большего, вклада...   \n",
       "1    [уверенностью, сказать, полезное, хорошее, сов...   \n",
       "2    [славяне, должны, работать, нам, нужны, могут,...   \n",
       "3    [русское, государство, росло, развивалось, сво...   \n",
       "4                 [поставим, власть, контроль, народа]   \n",
       "..                                                 ...   \n",
       "304  [социальное, соответствующий, подход, предпола...   \n",
       "305  [рост, продуктивности, человеческой, деятельно...   \n",
       "306  [социальная, дифференциация, числе, этническом...   \n",
       "307  [демографический, рост, расширение, экономичес...   \n",
       "308  [соответственно, политика, возникает, связи, у...   \n",
       "\n",
       "                                                medVec  \n",
       "0    [-0.040627204, -0.015365396, -0.047234748, -0....  \n",
       "1    [-0.039338265, -0.028398223, -0.0673505, -0.01...  \n",
       "2    [-0.036592077, -0.06395722, -0.045187872, -0.0...  \n",
       "3    [-0.01085669, -0.022112658, -0.07141073, -0.03...  \n",
       "4    [-0.052434314, -0.07669518, -0.019612849, -0.0...  \n",
       "..                                                 ...  \n",
       "304  [-0.03102859, -0.050743006, -0.037808374, -0.0...  \n",
       "305  [-0.022473654, -0.052780617, -0.06827721, -0.0...  \n",
       "306  [-0.017352613, -0.019620007, -0.04677039, -0.0...  \n",
       "307  [-0.03736097, -0.06539034, -0.062378306, -0.05...  \n",
       "308  [-0.048907045, -0.052896462, -0.05555214, -0.0...  \n",
       "\n",
       "[309 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized']=df['text'].apply(tokenize)\n",
    "df['medVec']=df['tokenized'].apply(mediumVector)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(df['medVec'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab': 0.0 mins\n",
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=10,\n",
    "    window=10,\n",
    "    size=300,\n",
    "    negative=10,\n",
    "    alpha=0.5,\n",
    "    min_alpha=0.0007,\n",
    "    sample=6e-5,\n",
    "    sg=1,workers=4)\n",
    "\n",
    "#BUILD_VOCAB()\n",
    "t = time.time()\n",
    "w2v_model.build_vocab(data, progress_per=100)\n",
    "print(\"Time to build vocab': {} mins\".format(round((time.time() - t) / 60, 2)))\n",
    "\n",
    "#TRAIN()\n",
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.wv.save_word2vec_format('hachi.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1616507f760>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = w2v_model.wv.load_word2vec_format(r'models\\hachi.bin', binary=True)\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'хуй' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-aee95fccbd65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"хуй\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"русских\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'хуй' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model1.most_similar(positive=[\"хуй\",\"русских\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-37b38b84b612>:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  index2word_set = set(model1.wv.index2word)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "index2word_set = set(model1.wv.index2word)\n",
    "\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# build a model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=10000))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(50,activation = 'relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimiz = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = optimiz ,metrics = ['accuracy'])\n",
    "\n",
    "# convert texts to TF-IDF matrices\n",
    "\n",
    "\n",
    "# fit the model\n",
    "#hist = model.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 5,batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listok=['хач','убивай ',\"негр\"]\n",
    "deist='всех'\n",
    "score=[]\n",
    "slovar={}\n",
    "for i in range(len(listok)):\n",
    "    s1_afv = avg_feature_vector(deist, model=model1, num_features=300, index2word_set=index2word_set)\n",
    "    s2_afv = avg_feature_vector(listok[i], model=model1, num_features=300, index2word_set=index2word_set)\n",
    "    sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "    score.append(sim)\n",
    "    slovar.update({deist+' / '+listok[i]:sim})\n",
    "sorted_tuples = sorted(slovar.items(), key=lambda item: item[1])\n",
    "\n",
    "sorted_dict = {k: v for k, v in sorted_tuples}\n",
    "\n",
    "print(sorted_dict)  # {1: 1, 3: 4, 2: 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
