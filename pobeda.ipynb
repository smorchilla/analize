{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhiti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from scipy import spatial\n",
    "import time\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel(r\"datasets/extremism.xlsx\")\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['id'] = df1['id']\n",
    "df['text'] = df1['text']\n",
    "df['class'] = df1['class']\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Таким образом никто не внес большего вклад...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Можно с уверенностью сказать, что все полезное...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Славяне должны на нас работать. Если они нам б...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Русское государство росло, развивалось из свои...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Поставим власть под контроль народа</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>305</td>\n",
       "      <td>Социальное. Соответствующий подход предполагае...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>306</td>\n",
       "      <td>Рост продуктивности человеческой деятельности ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>307</td>\n",
       "      <td>Социальная дифференциация, в том числе по этни...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>308</td>\n",
       "      <td>Демографический рост и расширение экономическо...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>309</td>\n",
       "      <td>Соответственно, политика возникает в связи с у...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  class\n",
       "0      1      Таким образом никто не внес большего вклад...    3.0\n",
       "1      2  Можно с уверенностью сказать, что все полезное...    3.0\n",
       "2      3  Славяне должны на нас работать. Если они нам б...    3.0\n",
       "3      4  Русское государство росло, развивалось из свои...    3.0\n",
       "4      5                Поставим власть под контроль народа    3.0\n",
       "..   ...                                                ...    ...\n",
       "304  305  Социальное. Соответствующий подход предполагае...    0.0\n",
       "305  306  Рост продуктивности человеческой деятельности ...    0.0\n",
       "306  307  Социальная дифференциация, в том числе по этни...    0.0\n",
       "307  308  Демографический рост и расширение экономическо...    0.0\n",
       "308  309  Соответственно, политика возникает в связи с у...    0.0\n",
       "\n",
       "[309 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_model = w2v_model.wv.load_word2vec_format(r'models/hachi.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('чурок', 0.9904348850250244),\n",
       " ('хачам', 0.9889359474182129),\n",
       " ('убивайте', 0.9868521690368652),\n",
       " ('россии', 0.9765247106552124),\n",
       " ('будут', 0.9525325298309326),\n",
       " ('убей', 0.8711485862731934),\n",
       " ('снова', 0.7811889052391052),\n",
       " ('нам', 0.725294828414917),\n",
       " ('своей', 0.7228884696960449),\n",
       " ('днем', 0.7154790759086609)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_model.most_similar('хачи')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = \"[0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "\n",
    "def comma2space(word):\n",
    "    new_word = ''\n",
    "    try:\n",
    "        for char in word:\n",
    "            if char in patterns:\n",
    "                char = ' '\n",
    "            new_word += char\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return new_word\n",
    "    \n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, '', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token:\n",
    "            \n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def dropstopwords(docs):\n",
    "    stopwords = nltk.corpus.stopwords.words('russian')\n",
    "    sen =  []\n",
    "    for doc in docs:\n",
    "        if (doc not in stopwords):\n",
    "            sen.append(doc)\n",
    "    return sen\n",
    "\n",
    "def dropcomma(doc):\n",
    "    new_sen = ''\n",
    "    for char in doc:\n",
    "        if (char not in patterns):\n",
    "            new_sen += char\n",
    "    return new_sen\n",
    "\n",
    "def tokenize(docs):\n",
    "    docs = comma2space(docs)\n",
    "    docs = lemmatize(docs)\n",
    "    docs = dropstopwords(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium word-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediumVector(sen):\n",
    "    medVec = exp_model[sen[0]]\n",
    "    unexceptedVectors = 0\n",
    "    for word in sen:\n",
    "        vec = False\n",
    "        try:\n",
    "            vec = exp_model.word_vec(word)\n",
    "            medVec = medVec + vec\n",
    "        except:\n",
    "            unexceptedVectors += 1\n",
    "    if unexceptedVectors != 0 :\n",
    "        print('Lost - ' + str(unexceptedVectors))\n",
    "    elif unexceptedVectors == len(sen):\n",
    "        print('Sentence lost')\n",
    "        \n",
    "    medVec = medVec - exp_model[sen[0]]\n",
    "    medVec = medVec / (len(sen) - unexceptedVectors)\n",
    "    return medVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [таким, образом, никто, внес, большего, вклада...\n",
       "1      [уверенностью, сказать, полезное, хорошее, сов...\n",
       "2      [славяне, должны, работать, нам, нужны, могут,...\n",
       "3      [русское, государство, росло, развивалось, сво...\n",
       "4                   [поставим, власть, контроль, народа]\n",
       "                             ...                        \n",
       "304    [социальное, соответствующий, подход, предпола...\n",
       "305    [рост, продуктивности, человеческой, деятельно...\n",
       "306    [социальная, дифференциация, числе, этническом...\n",
       "307    [демографический, рост, расширение, экономичес...\n",
       "308    [соответственно, политика, возникает, связи, у...\n",
       "Name: text, Length: 309, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df['text'].apply(tokenize)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>medVec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Таким образом никто не внес большего вклад...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[таким, образом, никто, внес, большего, вклада...</td>\n",
       "      <td>[-0.040627204, -0.015365397, -0.047234744, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Можно с уверенностью сказать, что все полезное...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[уверенностью, сказать, полезное, хорошее, сов...</td>\n",
       "      <td>[-0.03933827, -0.028398223, -0.06735049, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Славяне должны на нас работать. Если они нам б...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[славяне, должны, работать, нам, нужны, могут,...</td>\n",
       "      <td>[-0.03659208, -0.06395723, -0.045187864, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Русское государство росло, развивалось из свои...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[русское, государство, росло, развивалось, сво...</td>\n",
       "      <td>[-0.010856689, -0.022112656, -0.07141073, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Поставим власть под контроль народа</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[поставим, власть, контроль, народа]</td>\n",
       "      <td>[-0.052434314, -0.07669519, -0.01961285, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>305</td>\n",
       "      <td>Социальное. Соответствующий подход предполагае...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[социальное, соответствующий, подход, предпола...</td>\n",
       "      <td>[-0.031028584, -0.050743006, -0.037808374, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>306</td>\n",
       "      <td>Рост продуктивности человеческой деятельности ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[рост, продуктивности, человеческой, деятельно...</td>\n",
       "      <td>[-0.022473656, -0.05278062, -0.06827722, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>307</td>\n",
       "      <td>Социальная дифференциация, в том числе по этни...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[социальная, дифференциация, числе, этническом...</td>\n",
       "      <td>[-0.01735261, -0.01962001, -0.04677039, -0.027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>308</td>\n",
       "      <td>Демографический рост и расширение экономическо...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[демографический, рост, расширение, экономичес...</td>\n",
       "      <td>[-0.037360966, -0.06539034, -0.062378313, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>309</td>\n",
       "      <td>Соответственно, политика возникает в связи с у...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[соответственно, политика, возникает, связи, у...</td>\n",
       "      <td>[-0.048907053, -0.052896462, -0.055552132, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  class  \\\n",
       "0      1      Таким образом никто не внес большего вклад...    3.0   \n",
       "1      2  Можно с уверенностью сказать, что все полезное...    3.0   \n",
       "2      3  Славяне должны на нас работать. Если они нам б...    3.0   \n",
       "3      4  Русское государство росло, развивалось из свои...    3.0   \n",
       "4      5                Поставим власть под контроль народа    3.0   \n",
       "..   ...                                                ...    ...   \n",
       "304  305  Социальное. Соответствующий подход предполагае...    0.0   \n",
       "305  306  Рост продуктивности человеческой деятельности ...    0.0   \n",
       "306  307  Социальная дифференциация, в том числе по этни...    0.0   \n",
       "307  308  Демографический рост и расширение экономическо...    0.0   \n",
       "308  309  Соответственно, политика возникает в связи с у...    0.0   \n",
       "\n",
       "                                             tokenized  \\\n",
       "0    [таким, образом, никто, внес, большего, вклада...   \n",
       "1    [уверенностью, сказать, полезное, хорошее, сов...   \n",
       "2    [славяне, должны, работать, нам, нужны, могут,...   \n",
       "3    [русское, государство, росло, развивалось, сво...   \n",
       "4                 [поставим, власть, контроль, народа]   \n",
       "..                                                 ...   \n",
       "304  [социальное, соответствующий, подход, предпола...   \n",
       "305  [рост, продуктивности, человеческой, деятельно...   \n",
       "306  [социальная, дифференциация, числе, этническом...   \n",
       "307  [демографический, рост, расширение, экономичес...   \n",
       "308  [соответственно, политика, возникает, связи, у...   \n",
       "\n",
       "                                                medVec  \n",
       "0    [-0.040627204, -0.015365397, -0.047234744, -0....  \n",
       "1    [-0.03933827, -0.028398223, -0.06735049, -0.01...  \n",
       "2    [-0.03659208, -0.06395723, -0.045187864, -0.03...  \n",
       "3    [-0.010856689, -0.022112656, -0.07141073, -0.0...  \n",
       "4    [-0.052434314, -0.07669519, -0.01961285, -0.03...  \n",
       "..                                                 ...  \n",
       "304  [-0.031028584, -0.050743006, -0.037808374, -0....  \n",
       "305  [-0.022473656, -0.05278062, -0.06827722, -0.03...  \n",
       "306  [-0.01735261, -0.01962001, -0.04677039, -0.027...  \n",
       "307  [-0.037360966, -0.06539034, -0.062378313, -0.0...  \n",
       "308  [-0.048907053, -0.052896462, -0.055552132, -0....  \n",
       "\n",
       "[309 rows x 5 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'таким' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-f667924f6c89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'medVec'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmediumVector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-119-1833a2a250f6>\u001b[0m in \u001b[0;36mmediumVector\u001b[1;34m(sen)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmediumVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmedVec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0munexceptedVectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'таким' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "df['tokenized']=df['text'].apply(tokenize)\n",
    "df['medVec']=df['tokenized'].apply(mediumVector)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab': 0.0 mins\n",
      "Time to train the model: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=10,\n",
    "    window=10,\n",
    "    size=300,\n",
    "    negative=10,\n",
    "    alpha=0.5,\n",
    "    min_alpha=0.0007,\n",
    "    sample=6e-5,\n",
    "    sg=1,workers=4)\n",
    "\n",
    "#BUILD_VOCAB()\n",
    "t = time.time()\n",
    "w2v_model.build_vocab(data, progress_per=100)\n",
    "print(\"Time to build vocab': {} mins\".format(round((time.time() - t) / 60, 2)))\n",
    "\n",
    "#TRAIN()\n",
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.wv.save_word2vec_format('hachi.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-f66df250e90e>:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model1.syn0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.04750724, -0.09294917,  0.04646128, ..., -0.02326778,\n",
       "        -0.02748158,  0.04694638],\n",
       "       [-0.16215153, -0.03675573,  0.03022764, ...,  0.01612221,\n",
       "         0.01322101, -0.03014729],\n",
       "       [-0.05710144, -0.07443115,  0.06365845, ...,  0.00222445,\n",
       "         0.00118704,  0.08087655],\n",
       "       ...,\n",
       "       [-0.12255844, -0.10139921, -0.03399283, ..., -0.02904652,\n",
       "         0.03170789, -0.04719374],\n",
       "       [-0.07217082, -0.05173732,  0.02180016, ...,  0.03493413,\n",
       "        -0.015422  , -0.08324368],\n",
       "       [-0.04941457, -0.06038683,  0.01956714, ..., -0.01193344,\n",
       "        -0.02067705,  0.00612094]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = w2v_model.wv.load_word2vec_format(r'C:\\Users\\Zero\\hachi.bin', binary=True)\n",
    "model1.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('будут', 0.893851637840271),\n",
       " ('россии', 0.874309778213501),\n",
       " ('хачам', 0.8688006401062012),\n",
       " ('чурок', 0.8652452230453491),\n",
       " ('убивайте', 0.8409756422042847),\n",
       " ('умереть', 0.7862440943717957),\n",
       " ('самоубийства', 0.7742125988006592),\n",
       " ('знаю', 0.7616627216339111),\n",
       " ('это', 0.7607013583183289),\n",
       " ('думаю', 0.7601228356361389)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.most_similar(positive=[\"смерть\",\"хачи\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-37b38b84b612>:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  index2word_set = set(model1.wv.index2word)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "index2word_set = set(model1.wv.index2word)\n",
    "\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'всех / хач': nan, 'всех / убивай ': nan, 'всех / негр': nan}\n"
     ]
    }
   ],
   "source": [
    "listok=['хач','убивай ',\"негр\"]\n",
    "deist='всех'\n",
    "score=[]\n",
    "slovar={}\n",
    "for i in range(len(listok)):\n",
    "    s1_afv = avg_feature_vector(deist, model=model1, num_features=300, index2word_set=index2word_set)\n",
    "    s2_afv = avg_feature_vector(listok[i], model=model1, num_features=300, index2word_set=index2word_set)\n",
    "    sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "    score.append(sim)\n",
    "    slovar.update({deist+' / '+listok[i]:sim})\n",
    "sorted_tuples = sorted(slovar.items(), key=lambda item: item[1])\n",
    "\n",
    "sorted_dict = {k: v for k, v in sorted_tuples}\n",
    "\n",
    "print(sorted_dict)  # {1: 1, 3: 4, 2: 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
