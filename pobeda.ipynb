{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Zero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel(r\"datasets/extremism.xlsx\")\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['id'] = df1['id']\n",
    "df['text'] = df1['text']\n",
    "df['class'] = df1['class']\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Таким образом никто не внес большего вклад...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Можно с уверенностью сказать, что все полезное...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Славяне должны на нас работать. Если они нам б...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Русское государство росло, развивалось из свои...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Поставим власть под контроль народа</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>305</td>\n",
       "      <td>Социальное. Соответствующий подход предполагае...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>306</td>\n",
       "      <td>Рост продуктивности человеческой деятельности ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>307</td>\n",
       "      <td>Социальная дифференциация, в том числе по этни...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>308</td>\n",
       "      <td>Демографический рост и расширение экономическо...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>309</td>\n",
       "      <td>Соответственно, политика возникает в связи с у...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  class\n",
       "0      1      Таким образом никто не внес большего вклад...    3.0\n",
       "1      2  Можно с уверенностью сказать, что все полезное...    3.0\n",
       "2      3  Славяне должны на нас работать. Если они нам б...    3.0\n",
       "3      4  Русское государство росло, развивалось из свои...    3.0\n",
       "4      5                Поставим власть под контроль народа    3.0\n",
       "..   ...                                                ...    ...\n",
       "304  305  Социальное. Соответствующий подход предполагае...    0.0\n",
       "305  306  Рост продуктивности человеческой деятельности ...    0.0\n",
       "306  307  Социальная дифференциация, в том числе по этни...    0.0\n",
       "307  308  Демографический рост и расширение экономическо...    0.0\n",
       "308  309  Соответственно, политика возникает в связи с у...    0.0\n",
       "\n",
       "[309 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = \"[0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "def comma2space(word):\n",
    "    new_word = ''\n",
    "    try:\n",
    "        for char in word:\n",
    "            if char in patterns:\n",
    "                char = ' '\n",
    "            new_word += char\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return new_word\n",
    "    \n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, '', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token:\n",
    "            \n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def dropstopwords(docs):\n",
    "    stopwords = nltk.corpus.stopwords.words('russian')\n",
    "    sen =  []\n",
    "    for doc in docs:\n",
    "        if (doc not in stopwords):\n",
    "            sen.append(doc)\n",
    "    return sen\n",
    "\n",
    "def dropcomma(doc):\n",
    "    new_sen = ''\n",
    "    for char in doc:\n",
    "        if (char not in patterns):\n",
    "            new_sen += char\n",
    "    return new_sen\n",
    "def tokenize(docs):\n",
    "    docs = comma2space(docs)\n",
    "    docs = lemmatize(docs)\n",
    "    docs = dropstopwords(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [таким, образом, никто, внес, большего, вклада...\n",
       "1      [уверенностью, сказать, полезное, хорошее, сов...\n",
       "2      [славяне, должны, работать, нам, нужны, могут,...\n",
       "3      [русское, государство, росло, развивалось, сво...\n",
       "4                   [поставим, власть, контроль, народа]\n",
       "                             ...                        \n",
       "304    [социальное, соответствующий, подход, предпола...\n",
       "305    [рост, продуктивности, человеческой, деятельно...\n",
       "306    [социальная, дифференциация, числе, этническом...\n",
       "307    [демографический, рост, расширение, экономичес...\n",
       "308    [соответственно, политика, возникает, связи, у...\n",
       "Name: text, Length: 309, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df['text'].apply(tokenize)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab': 0.0 mins\n",
      "Time to train the model: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "#WORD2VEC()\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=10,\n",
    "    window=10,\n",
    "    size=300,\n",
    "    negative=10,\n",
    "    alpha=0.5,\n",
    "    min_alpha=0.0007,\n",
    "    sample=6e-5,\n",
    "    sg=1,workers=4)\n",
    "\n",
    "#BUILD_VOCAB()\n",
    "t = time.time()\n",
    "w2v_model.build_vocab(data, progress_per=100)\n",
    "print(\"Time to build vocab': {} mins\".format(round((time.time() - t) / 60, 2)))\n",
    "\n",
    "#TRAIN()\n",
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.wv.save_word2vec_format('hachi.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-f66df250e90e>:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model1.syn0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.04750724, -0.09294917,  0.04646128, ..., -0.02326778,\n",
       "        -0.02748158,  0.04694638],\n",
       "       [-0.16215153, -0.03675573,  0.03022764, ...,  0.01612221,\n",
       "         0.01322101, -0.03014729],\n",
       "       [-0.05710144, -0.07443115,  0.06365845, ...,  0.00222445,\n",
       "         0.00118704,  0.08087655],\n",
       "       ...,\n",
       "       [-0.12255844, -0.10139921, -0.03399283, ..., -0.02904652,\n",
       "         0.03170789, -0.04719374],\n",
       "       [-0.07217082, -0.05173732,  0.02180016, ...,  0.03493413,\n",
       "        -0.015422  , -0.08324368],\n",
       "       [-0.04941457, -0.06038683,  0.01956714, ..., -0.01193344,\n",
       "        -0.02067705,  0.00612094]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = w2v_model.wv.load_word2vec_format(r'C:\\Users\\Zero\\hachi.bin', binary=True)\n",
    "model1.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('будут', 0.893851637840271),\n",
       " ('россии', 0.874309778213501),\n",
       " ('хачам', 0.8688006401062012),\n",
       " ('чурок', 0.8652452230453491),\n",
       " ('убивайте', 0.8409756422042847),\n",
       " ('умереть', 0.7862440943717957),\n",
       " ('самоубийства', 0.7742125988006592),\n",
       " ('знаю', 0.7616627216339111),\n",
       " ('это', 0.7607013583183289),\n",
       " ('думаю', 0.7601228356361389)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.most_similar(positive=[\"смерть\",\"хачи\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-37b38b84b612>:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  index2word_set = set(model1.wv.index2word)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "index2word_set = set(model1.wv.index2word)\n",
    "\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'всех / хач': nan, 'всех / убивай ': nan, 'всех / негр': nan}\n"
     ]
    }
   ],
   "source": [
    "listok=['хач','убивай ',\"негр\"]\n",
    "deist='всех'\n",
    "score=[]\n",
    "slovar={}\n",
    "for i in range(len(listok)):\n",
    "    s1_afv = avg_feature_vector(deist, model=model1, num_features=300, index2word_set=index2word_set)\n",
    "    s2_afv = avg_feature_vector(listok[i], model=model1, num_features=300, index2word_set=index2word_set)\n",
    "    sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "    score.append(sim)\n",
    "    slovar.update({deist+' / '+listok[i]:sim})\n",
    "sorted_tuples = sorted(slovar.items(), key=lambda item: item[1])\n",
    "\n",
    "sorted_dict = {k: v for k, v in sorted_tuples}\n",
    "\n",
    "print(sorted_dict)  # {1: 1, 3: 4, 2: 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
