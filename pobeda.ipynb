{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhiti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel(r\"datasets/extremism.xlsx\")\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['id'] = df1['id']\n",
    "df['text'] = df1['text']\n",
    "df['class'] = df1['class']\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Таким образом никто не внес большего вклад...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Можно с уверенностью сказать, что все полезное...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Славяне должны на нас работать. Если они нам б...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Русское государство росло, развивалось из свои...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Поставим власть под контроль народа</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  class\n",
       "0      1      Таким образом никто не внес большего вклад...    3.0\n",
       "1      2  Можно с уверенностью сказать, что все полезное...    3.0\n",
       "2      3  Славяне должны на нас работать. Если они нам б...    3.0\n",
       "3      4  Русское государство росло, развивалось из свои...    3.0\n",
       "4      5                Поставим власть под контроль народа    3.0\n",
       "..   ...                                                ...    ...\n",
       "335  336                                                NaN    NaN\n",
       "336  337                                                NaN    NaN\n",
       "337  338                                                NaN    NaN\n",
       "338  339                                                NaN    NaN\n",
       "339  340                                                NaN    NaN\n",
       "\n",
       "[340 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comma2space(word):\n",
    "    new_word = ''\n",
    "    try:\n",
    "        for char in word:\n",
    "            if char in patterns:\n",
    "                char = ' '\n",
    "            new_word += char\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return new_word\n",
    "    \n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, '', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token:\n",
    "            \n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def dropstopwords(docs):\n",
    "    stopwords = nltk.corpus.stopwords.words('russian')\n",
    "    sen =  []\n",
    "    for doc in docs:\n",
    "        if (doc not in stopwords):\n",
    "            sen.append(doc)\n",
    "    return sen\n",
    "\n",
    "def dropcomma(doc):\n",
    "    new_sen = ''\n",
    "    for char in doc:\n",
    "        if (char not in patterns):\n",
    "            new_sen += char\n",
    "    return new_sen\n",
    "def tokenize(docs):\n",
    "    docs = comma2space(docs)\n",
    "    docs = lemmatize(docs)\n",
    "    docs = dropstopwords(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14     [убивай, хача, мочи, хача, бей, черных, бей, х...\n",
       "59            [россия, русских, долой, инородную, мразь]\n",
       "92     [аллах, даруй, нам, стойкости, пока, увидим, н...\n",
       "116    [твою, боль, собой, заберу, могилу, твои, печа...\n",
       "197    [самоубийцы, считай, телесной, боли, руки, нак...\n",
       "                             ...                        \n",
       "178                         [такую, погоду, повеситься…]\n",
       "181    [никому, позволял, коснуться, большей, части, ...\n",
       "156    [лесу, веселых, трупов, мертвецки, пьян, брел,...\n",
       "146    [терять, страшно, заднем, фоне, взорвалась, мы...\n",
       "131    [думали, смерти, забыли, такое, счастье, верно...\n",
       "Name: text, Length: 201, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df['text'].apply(tokenize)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab': 0.0 mins\n",
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "#WORD2VEC()\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=5,\n",
    "    window=4,\n",
    "    size=30,\n",
    "    negative=10,\n",
    "    alpha=0.5,\n",
    "    min_alpha=0.0007,\n",
    "    sample=6e-5,\n",
    "    sg=1,workers=4)\n",
    "\n",
    "#BUILD_VOCAB()\n",
    "t = time.time()\n",
    "w2v_model.build_vocab(data, progress_per=100)\n",
    "print(\"Time to build vocab': {} mins\".format(round((time.time() - t) / 60, 2)))\n",
    "\n",
    "#TRAIN()\n",
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=300, report_delay=1)\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.wv.save_word2vec_format('hachi.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-c8ac0a104979>:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model1.syn0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.01329773, -0.04954648,  0.07695488, ...,  0.07561335,\n",
       "         0.12918606, -0.00850406],\n",
       "       [ 0.011624  , -0.01226313,  0.01542811, ...,  0.07488082,\n",
       "        -0.05080352,  0.05965197],\n",
       "       [-0.05951629, -0.06781738,  0.01338605, ...,  0.07335977,\n",
       "         0.12016334,  0.05530556],\n",
       "       ...,\n",
       "       [ 0.01125513,  0.02318161,  0.02822547, ...,  0.03792449,\n",
       "         0.00100433,  0.0697925 ],\n",
       "       [ 0.03417982, -0.04482038,  0.07471242, ...,  0.0877353 ,\n",
       "         0.02725162,  0.05327835],\n",
       "       [-0.01022633, -0.04199966,  0.05124441, ...,  0.09970852,\n",
       "         0.02095896,  0.0199973 ]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = w2v_model.wv.load_word2vec_format(r'C:\\Users\\Zero\\exp_model.bin', binary=True)\n",
    "model1.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('короткая', 0.8679245114326477),\n",
       " ('любая', 0.8675863146781921),\n",
       " ('светлая', 0.8640148639678955),\n",
       " ('идиотская', 0.8541393280029297),\n",
       " ('нравилась', 0.8533639311790466),\n",
       " ('душевная', 0.849618673324585),\n",
       " ('великолепная', 0.8489678502082825),\n",
       " ('лапочка', 0.8488980531692505),\n",
       " ('удивительная', 0.847754716873169),\n",
       " ('важная', 0.8469743728637695)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.most_similar(positive=[\"смерть\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-88-37b38b84b612>:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  index2word_set = set(model1.wv.index2word)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "index2word_set = set(model1.wv.index2word)\n",
    "\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'хочу умереть / прыгунть в окно': 0.4668493866920471, 'хочу умереть / валить пидоров': 0.64153653383255, 'хочу умереть / разбежаться в пропасть': 0.6599088311195374}\n"
     ]
    }
   ],
   "source": [
    "listok=['прыгунть в окно','разбежаться в пропасть',\"валить пидоров\"]\n",
    "deist='хочу умереть'\n",
    "score=[]\n",
    "slovar={}\n",
    "for i in range(len(listok)):\n",
    "    s1_afv = avg_feature_vector(deist, model=model1, num_features=300, index2word_set=index2word_set)\n",
    "    s2_afv = avg_feature_vector(listok[i], model=model1, num_features=300, index2word_set=index2word_set)\n",
    "    sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "    score.append(sim)\n",
    "    slovar.update({deist+' / '+listok[i]:sim})\n",
    "sorted_tuples = sorted(slovar.items(), key=lambda item: item[1])\n",
    "\n",
    "sorted_dict = {k: v for k, v in sorted_tuples}\n",
    "\n",
    "print(sorted_dict)  # {1: 1, 3: 4, 2: 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
